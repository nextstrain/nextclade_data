#!/usr/bin/env python3

"""
Builds a fresh data repo from source data
"""

import fnmatch
import json
import os
import shutil
from collections import defaultdict


from typing import List

THIS_DIR = os.path.dirname(os.path.realpath(__file__))
PROJECT_ROOT_DIR = os.path.realpath(os.path.join(THIS_DIR, ".."))

# Source data will be taken from this directory (relative to the project root)
# Committed to repository. Modify files there.
DATA_INPUT_DIR = os.path.realpath(os.path.join(PROJECT_ROOT_DIR, "data"))

# Final, uncompressed datasets will be generated in this directory (relative to the project root)
# It is gitignored and is safe to delete. Do not modify manually.
DATA_OUTPUT_DIR = os.path.realpath(os.path.join(PROJECT_ROOT_DIR, "data_output"))

SETTINGS_JSON_PATH = os.path.realpath(os.path.join(DATA_INPUT_DIR, "settings.json"))
INDEX_JSON_PATH = os.path.realpath(os.path.join(DATA_OUTPUT_DIR, "index.json"))
INDEX_V2_JSON_PATH = os.path.realpath(os.path.join(DATA_OUTPUT_DIR, "index_v2.json"))
NOT_FOUND_JSON_PATH = os.path.realpath(os.path.join(DATA_OUTPUT_DIR, "404.json"))

# In case we want to change these words
REFERENCE = "reference"
REFERENCES = "references"

from collections import namedtuple


def dict_to_namedtuple(name, dic):
    return namedtuple(name, dic.keys())(*dic.values())


def find_files(pattern, here):
    for path, dirs, files in os.walk(os.path.abspath(here)):
        for filename in fnmatch.filter(files, pattern):
            yield os.path.join(path, filename)


def find_dirs(here):
    for path, dirs, _ in os.walk(os.path.abspath(here)):
        for dirr in dirs:
            yield os.path.join(path, dirr)


def find_dirs_here(here):
    return filter(os.path.isdir, [os.path.join(here, e) for e in os.listdir(here)])


def json_write(obj, filepath):
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, "w") as f:
        json.dump(obj, f, indent=2, sort_keys=True)
        f.write("\n")


def get_paths(dataset_json, dataset_ref_json, version):
    dataset_name = dataset_json["name"]
    ref_accession = dataset_ref_json[REFERENCE]["accession"]
    version_tag = version["tag"]
    versions_dir = f"datasets/{dataset_name}/{REFERENCES}/{ref_accession}/versions"
    files_dir = f"{versions_dir}/{version_tag}/files"
    filenames = version['files'].copy()
    filenames["tag"] = "tag.json"
    files = {f"{filetype}": f"/{files_dir}/{filename}" for filetype, filename in filenames.items()}

    input_files_dir_abs = os.path.realpath(os.path.join(DATA_INPUT_DIR, files_dir))
    output_files_dir_abs = os.path.realpath(os.path.join(DATA_OUTPUT_DIR, files_dir))

    zip_dir = f"{versions_dir}/{version_tag}/zip-bundle"
    zip_base = f"nextclade_dataset_{dataset_name}_{ref_accession}_{version_tag}"
    zip_filename = f"{zip_base}.zip"
    zip_bundle_url = f"/{zip_dir}/{zip_filename}"

    zip_base_path = os.path.join(DATA_OUTPUT_DIR, zip_dir, zip_base)
    zip_src_dir = os.path.realpath(os.path.join(DATA_OUTPUT_DIR, files_dir))

    return dict_to_namedtuple("paths", {
        "files": files,
        "versions_dir": versions_dir,
        "input_files_dir_abs": input_files_dir_abs,
        "output_files_dir_abs": output_files_dir_abs,
        "zip_base_path": zip_base_path,
        "zip_src_dir": zip_src_dir,
        "zip_bundle_url": zip_bundle_url
    })


def copy_dataset_version_files(version, src_dir, dst_dir):
    os.makedirs(dst_dir, exist_ok=True)
    for _, filename in version['files'].items():
        input_filepath = os.path.join(src_dir, filename)
        output_filepath = os.path.join(dst_dir, filename)
        shutil.copy2(input_filepath, output_filepath)


def make_zip_bundle(dataset_json, dataset_ref_json, version):
    paths = get_paths(dataset_json, dataset_ref_json, version)
    os.makedirs(os.path.dirname(paths.zip_base_path), exist_ok=True)
    shutil.make_archive(
        base_name=paths.zip_base_path,
        format='zip',
        root_dir=paths.zip_src_dir
    )

def build_dataset_v1(dataset_json, dataset_refs):
    dataset = {**dataset_json, "datasetRefs": dataset_refs}
    return dataset

# Ordering of datasets
position = {
  'sars-cov-2': 4,
  'MPXV': 3,
  'hMPXV': 2,
  'hMPXV_B1': 1,
  'sars-cov-2-no-recomb': -1000
}
position = defaultdict(int, position)

def postprocess_datasets_v1(datasets, settings):
    # mpx datasets are not compatible with v1
    # TODO: should probably filter by version instead, but it is a bit more involved due to nested dict structure
    datasets = list(filter(lambda dataset: "mpx" not in dataset["name"].lower(), datasets))

    # Sort datasets alphabetically
    datasets.sort(key=lambda x: x["nameFriendly"],reverse=True)
    # Move monkeypox to the front, if it exists
    datasets.sort(key=lambda x: position[x["name"]])
    # Move default dataset to the beginning
    datasets.sort(key=lambda x: x["name"] == defaultDatasetName, reverse=True)

    datasets_json = dict()
    datasets_json.update({"settings": settings})
    datasets_json.update({"datasets": datasets})

    return datasets_json


def build_dataset_v2(dataset_json, dataset_ref):
    # HACK: converts from old hierarchical object format to flat attributes
    # TODO: decide how to reorganize the hierarchical objects and folder structure in the git repo and rewrite this
    attributes = {
        "name": {
            "value": dataset_name,
            "valueFriendly": dataset_json["nameFriendly"],
            "isDefault": dataset_name == defaultDatasetName
        },
        "reference": {
            "value": dataset_ref["reference"]["accession"],
            "valueFriendly": dataset_ref["reference"]["strainName"],
            "isDefault": dataset_ref["reference"]["accession"] == dataset_json["defaultRef"]
        },
        "tag": {
            "value": dataset_ref["tag"],
            "isDefault": dataset_ref["latest"]
        },
    }

    params = {
        "defaultGene": dataset_json.get("defaultGene"),
        "geneOrderPreference": dataset_json.get("geneOrderPreference"),
    }

    params = { k: v for k, v in params.items() if v is not None }

    # TODO: figure out where 'enabled' field should come from
    dataset = {**dataset_json, **dataset_ref, "attributes": attributes, "params": params}

    # HACK: converts files to the new format
    # TODO: modify original dataset files instead
    def convert_files(files):
        def get_filename(url):
            return url.split("/")[-1]

        def remove_leading_slash(url):
            return url.lstrip("/")

        return {get_filename(url): remove_leading_slash(url) for _, url in files.items()}

    dataset["files"] = convert_files(dataset["files"])

    # HACK: omits properties that have been moved into the attributes just above
    dataset = {k: v for k, v in dataset.items() if k in [
        "enabled",
        "attributes",
        "comment",
        "compatibility",
        "files",
        "params",
        "zipBundle",
    ]}

    return dataset

def postprocess_datasets_v2(datasets_v2, _):
    datasets_v2.sort(key=lambda x: x["attributes"]["name"]["value"])
    datasets_v2.sort(key=lambda x: position[x["attributes"]["name"]["value"]], reverse=True)
    datasets_v2.sort(key=lambda x: x["attributes"]["name"]["value"] == "sars-cov-2", reverse=True)

    datasets_v2 = [ { "id": str(i), **dataset } for i, dataset in enumerate(datasets_v2) ]

    index_json = dict()
    index_json.update({"schema": "2.0.0"})
    index_json.update({"datasets": datasets_v2})

    return index_json


if __name__ == '__main__':
    shutil.rmtree(DATA_OUTPUT_DIR, ignore_errors=True)

    with open(SETTINGS_JSON_PATH, 'r') as f:
        settings_json = json.load(f)

    settings = settings_json
    defaultDatasetName = settings['defaultDatasetName']
    defaultDatasetNameFriendly = None

    datasets = []
    datasets_v2 = []
    for dataset_json_path in find_files(pattern="dataset.json", here=os.path.join(DATA_INPUT_DIR, "datasets")):
        with open(dataset_json_path, 'r') as f:
            dataset_json: dict = json.load(f)
        dataset_json_original = dataset_json.copy()

        dataset_name = dataset_json["name"]
        if dataset_name == defaultDatasetName:
            defaultDatasetNameFriendly = dataset_json['nameFriendly']

        dataset_dir = os.path.dirname(dataset_json_path)
        dataset_refs: List[dict] = []
        dataset_refs_v2: List[dict] = []
        for dataset_ref_json_path in find_files(pattern="datasetRef.json", here=dataset_dir):

            with open(dataset_ref_json_path, 'r') as f:
                dataset_ref_json: dict = json.load(f)

            dataset_ref_dir = os.path.dirname(dataset_ref_json_path)

            # Read data descriptions for the tags
            version_jsons: List[dict] = []
            for tag_path in find_files("tag.json", dataset_ref_dir):
                with open(tag_path, 'r') as f:
                    version_json: dict = json.load(f)
                version_jsons.append(version_json)

            del version_json
            version_jsons.sort(key=lambda x: x["tag"], reverse=True)

            for i, version_json in enumerate(version_jsons):
                tag = version_json["tag"]
                paths = get_paths(dataset_json, dataset_ref_json, version_json)

                # Generate `tag.json` inside output directory
                tag_metadata = {**version_json["metadata"], **dataset_json["metadata"], **dataset_ref_json["metadata"]}
                tag_json = {**version_json, **dataset_json, **dataset_ref_json, "metadata": tag_metadata}
                tag_json_path = os.path.join(paths.output_files_dir_abs, "tag.json")
                json_write(tag_json, tag_json_path)

                # Copy files, including `tag.json` into output directory
                copy_dataset_version_files(version_json, src_dir=paths.input_files_dir_abs,
                                           dst_dir=paths.output_files_dir_abs)

                # Zip output directory
                make_zip_bundle(dataset_json, dataset_ref_json, version_json)

                # Copy latest version output directory to directory `latest`
                # (assumes `version_jsons` are sorted by tag, reversed!)
                if i == 0:
                    this_version_dir = os.path.join(DATA_OUTPUT_DIR, paths.versions_dir, tag)
                    latest_version_dir = os.path.join(DATA_OUTPUT_DIR, paths.versions_dir, "latest")
                    shutil.copytree(this_version_dir, latest_version_dir)

                version_json.update({"files": paths.files, "zipBundle": paths.zip_bundle_url, "latest": i == 0})

            dataset_refs.append({**dataset_ref_json, "versions": version_jsons})

            for version_json in version_jsons:
                dataset_refs_v2.append({**dataset_ref_json, **version_json})

        datasets.append(build_dataset_v1(dataset_json, dataset_refs))

        for dataset_ref in dataset_refs_v2:
            datasets_v2.append(build_dataset_v2(dataset_json, dataset_ref))

    settings.update({'defaultDatasetNameFriendly': defaultDatasetNameFriendly})

    index_v1_json = postprocess_datasets_v1(datasets, settings)
    json_write(index_v1_json, INDEX_JSON_PATH)

    index_v2_json = postprocess_datasets_v2(datasets_v2, settings)
    json_write(index_v2_json, INDEX_V2_JSON_PATH)

    not_found_json = { "status": 404, "message": "Not found" }
    json_write(not_found_json, NOT_FOUND_JSON_PATH)
