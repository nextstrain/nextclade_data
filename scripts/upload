#!/usr/bin/env bash
set -euxo pipefail
shopt -s dotglob
trap "exit" INT

# Uploads fresh data repo to AWS S3

# Increase number of attempts AWS CLI does. This is useful for Cloudfront commands which have increased failure rates for no reason.
export AWS_MAX_ATTEMPTS=10

# Directory where this script resides
THIS_DIR=$(cd $(dirname "${BASH_SOURCE[0]}"); pwd)

# Where the source code is
PROJECT_ROOT_DIR="$(realpath ${THIS_DIR}/..)"

source "${PROJECT_ROOT_DIR}/.env.example"
if [ -f "${PROJECT_ROOT_DIR}/.env" ]; then
  source "${PROJECT_ROOT_DIR}/.env"
fi

if [ "${VERBOSE:=0}" == "1" ]; then
set -x
fi


export AWS_PAGER=''
DATA_OUTPUT_DIR="${PROJECT_ROOT_DIR}/data_output"
DATA_TEMP_DIR="${PROJECT_ROOT_DIR}/data_temp"

: "${DATA_AWS_S3_BUCKET:?The DATA_AWS_S3_BUCKET environment variable is required.}"
: "${DATA_AWS_CLOUDFRONT_DISTRIBUTION_ID:?The DATA_AWS_CLOUDFRONT_DISTRIBUTION_ID environment variable is required.}"


rm -rf "${DATA_TEMP_DIR}"
mkdir -p "${DATA_TEMP_DIR}"

rsync --archive --exclude='*.zip' "${DATA_OUTPUT_DIR}/" "${DATA_TEMP_DIR}/"

pushd "${DATA_TEMP_DIR}" >/dev/null
  pigz -kfrq .
  find . -type f \( ! -iname "*.gz" -a ! -iname "*.br" \) | parallel brotli -kf || true
popd >/dev/null



aws s3 sync --delete --only-show-errors \
  --cache-control "no-cache" \
  --metadata-directive REPLACE \
  --exclude "*" \
  --include "*.json" \
  --exclude "*/*" \
  "${DATA_TEMP_DIR}" "${DATA_AWS_S3_BUCKET}"

aws s3 sync --delete --only-show-errors \
  --cache-control "no-cache" \
  --content-encoding gzip \
  --metadata-directive REPLACE \
  --exclude "*" \
  --include "*.json.gz" \
  --exclude "*/*" \
  "${DATA_TEMP_DIR}" "${DATA_AWS_S3_BUCKET}"

aws s3 sync --delete --only-show-errors \
  --cache-control "no-cache" \
  --content-encoding br \
  --metadata-directive REPLACE \
  --exclude "*" \
  --include "*.json.br" \
  --exclude "*/*" \
  "${DATA_TEMP_DIR}" "${DATA_AWS_S3_BUCKET}"




aws s3 sync --delete --only-show-errors \
  --cache-control "public, max-age=86400" \
  --metadata-directive REPLACE \
  --exclude "*.br" \
  --exclude "*.gz" \
  --exclude "index*.json*" \
  --exclude "404.json*" \
  "${DATA_TEMP_DIR}" "${DATA_AWS_S3_BUCKET}"

aws s3 sync --delete --only-show-errors \
  --cache-control "public, max-age=86400" \
  --content-encoding gzip \
  --metadata-directive REPLACE \
  --exclude "*" \
  --include "*.gz" \
  --exclude "index*.json*" \
  --exclude "404.json*" \
  "${DATA_TEMP_DIR}" "${DATA_AWS_S3_BUCKET}"

aws s3 sync --delete --only-show-errors \
  --cache-control "public, max-age=86400" \
  --content-encoding br \
  --metadata-directive REPLACE \
  --exclude "*" \
  --include "*.br" \
  --exclude "index*.json*" \
  --exclude "404.json*" \
  "${DATA_TEMP_DIR}" "${DATA_AWS_S3_BUCKET}"




aws s3 sync --delete --only-show-errors \
  --cache-control "public, max-age=86400" \
  --metadata-directive REPLACE \
  --exclude "*" \
  --include "*.zip" \
  "${DATA_OUTPUT_DIR}" "${DATA_AWS_S3_BUCKET}"


aws cloudfront create-invalidation \
  --distribution-id "${DATA_AWS_CLOUDFRONT_DISTRIBUTION_ID}" \
  --paths "/*" >/dev/null
